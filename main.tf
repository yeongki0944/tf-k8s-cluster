# main.tf - Kubernetes Cluster Infrastructure

terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "~> 4.0"
    }
    random = {
      source  = "hashicorp/random"
      version = "~> 3.0"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.0"
    }
  }
}

# AWS Provider Configuration
provider "aws" {
  region  = var.aws_region
  profile = "mzadmin"
}

# Data source for availability zones
data "aws_availability_zones" "available" {
  state = "available"
}

# Data source for latest Amazon Linux 2023 AMI
data "aws_ami" "amazon_linux" {
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["al2023-ami-*-x86_64"]
  }

  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

# Random ID for cluster identification
resource "random_id" "cluster" {
  byte_length = 4
}

# Local values
locals {
  cluster_id = "${var.cluster_name}-${random_id.cluster.hex}"
  
  common_tags = {
    Environment = "learning"
    Project     = "kubernetes"
    Owner       = "yg"
    ManagedBy   = "terraform"
    ClusterName = var.cluster_name
    ClusterID   = local.cluster_id
  }
}

#===========================================
# VPC and Networking
#===========================================

# VPC
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-vpc"
  })
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-igw"
  })
}

# Public Subnets
resource "aws_subnet" "public" {
  count = length(var.public_subnet_cidrs)
  
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.public_subnet_cidrs[count.index]
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-public-subnet-${count.index + 1}"
    Type = "Public"
  })
}

# Route Table for Public Subnets
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-public-rt"
  })
}

# Route Table Association for Public Subnets
resource "aws_route_table_association" "public" {
  count = length(aws_subnet.public)
  
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

#===========================================
# Security Groups
#===========================================

# Security Group for Kubernetes Master Node
resource "aws_security_group" "k8s_master" {
  name_prefix = "${var.cluster_name}-master-sg"
  vpc_id      = aws_vpc.main.id

  # Allow all inbound traffic (for learning)
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All TCP traffic"
  }

  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All UDP traffic"
  }

  # All outbound traffic
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All outbound traffic"
  }

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-master-sg"
    Type = "Master"
  })
}

# Security Group for Kubernetes Worker Nodes
resource "aws_security_group" "k8s_worker" {
  name_prefix = "${var.cluster_name}-worker-sg"
  vpc_id      = aws_vpc.main.id

  # Allow all inbound traffic (for learning)
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All TCP traffic"
  }

  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "udp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All UDP traffic"
  }

  # All outbound traffic
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
    description = "All outbound traffic"
  }

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-worker-sg"
    Type = "Worker"
  })
}

#===========================================
# Key Pair Management
#===========================================

# Generate Private Key
resource "tls_private_key" "k8s_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

# Create AWS Key Pair
resource "aws_key_pair" "k8s_key" {
  key_name   = "${var.cluster_name}-key"
  public_key = tls_private_key.k8s_key.public_key_openssh

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-keypair"
  })
}

# Save Private Key to Local File
resource "local_file" "private_key" {
  content         = tls_private_key.k8s_key.private_key_pem
  filename        = "${path.module}/${var.cluster_name}-key.pem"
  file_permission = "0600"
}

#===========================================
# Parameter Store for Join Command
#===========================================

# Parameter Store for storing join command
resource "aws_ssm_parameter" "k8s_join_command" {
  name  = "/k8s/${local.cluster_id}/join-command"
  type  = "SecureString"
  value = "placeholder"

  lifecycle {
    ignore_changes = [value]
  }

  tags = merge(local.common_tags, {
    Name        = "${var.cluster_name}-join-command"
    Description = "Kubernetes cluster join command"
  })
}

#===========================================
# IAM Roles and Policies
#===========================================

# IAM Role for Kubernetes Nodes
resource "aws_iam_role" "k8s_nodes" {
  name = "${var.cluster_name}-nodes-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-nodes-role"
  })
}

# IAM Policy for SSM Parameter Store Access
resource "aws_iam_policy" "k8s_ssm_access" {
  name        = "${var.cluster_name}-ssm-access-policy"
  description = "Allow access to SSM Parameter Store for Kubernetes cluster"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ssm:GetParameter",
          "ssm:PutParameter"
        ]
        Resource = "arn:aws:ssm:${var.aws_region}:*:parameter/k8s/${local.cluster_id}/*"
      }
    ]
  })

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-ssm-policy"
  })
}

# Attach SSM Policy to Role
resource "aws_iam_role_policy_attachment" "k8s_ssm_access" {
  role       = aws_iam_role.k8s_nodes.name
  policy_arn = aws_iam_policy.k8s_ssm_access.arn
}

# Instance Profile for EC2
resource "aws_iam_instance_profile" "k8s_nodes" {
  name = "${var.cluster_name}-nodes-profile"
  role = aws_iam_role.k8s_nodes.name

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-nodes-profile"
  })
}

#===========================================
# Kubernetes Master Node
#===========================================

# Kubernetes Master Node
resource "aws_instance" "k8s_master" {
  ami                    = data.aws_ami.amazon_linux.id
  instance_type          = var.master_instance_type
  key_name               = aws_key_pair.k8s_key.key_name
  vpc_security_group_ids = [aws_security_group.k8s_master.id]
  subnet_id              = aws_subnet.public[0].id
  iam_instance_profile   = aws_iam_instance_profile.k8s_nodes.name

  root_block_device {
    volume_size           = var.disk_size
    volume_type           = "gp3"
    delete_on_termination = true
    encrypted             = false
  }

  metadata_options {
    http_tokens = "required"  # IMDSv2 only
  }

  user_data = base64encode(templatefile("${path.module}/scripts/k8s-master-init.sh", {
    cluster_id     = local.cluster_id
    parameter_name = aws_ssm_parameter.k8s_join_command.name
    aws_region     = var.aws_region
    cluster_name   = var.cluster_name
  }))

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-master"
    Type = "Master"
    Role = "ControlPlane"
  })
}

#===========================================
# Wait for Master Initialization Complete
#===========================================

# Wait for master node initialization to complete by monitoring Parameter Store
resource "null_resource" "wait_for_master_init" {
  depends_on = [aws_instance.k8s_master]

  provisioner "local-exec" {
    command = <<-EOT
      echo "=================================================="
      echo "ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ì™„ë£Œ ëŒ€ê¸° ì¤‘..."
      echo "Parameter Store ê²½ë¡œ: ${aws_ssm_parameter.k8s_join_command.name}"
      echo "ëª¨ë‹ˆí„°ë§ ê°„ê²©: 10ì´ˆ, ìµœëŒ€ ëŒ€ê¸° ì‹œê°„: 15ë¶„"
      echo "=================================================="
      
      for i in {1..90}; do
        echo "[$i/90] ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ ì¤‘... ($(date))"
        
        # Parameter Storeì—ì„œ ì¡°ì¸ ëª…ë ¹ì–´ í™•ì¸
        JOIN_CMD=$(aws ssm get-parameter \
            --region ${var.aws_region} \
            --name "${aws_ssm_parameter.k8s_join_command.name}" \
            --with-decryption \
            --profile mzadmin \
            --query 'Parameter.Value' \
            --output text 2>/dev/null)
        
        # ì¡°ì¸ ëª…ë ¹ì–´ ìœ íš¨ì„± í™•ì¸
        if [ $? -eq 0 ] && [ "$JOIN_CMD" != "placeholder" ] && [ ! -z "$JOIN_CMD" ] && echo "$JOIN_CMD" | grep -q "kubeadm join"; then
          echo "=================================================="
          echo "âœ… ë§ˆìŠ¤í„° ë…¸ë“œ ì´ˆê¸°í™” ì™„ë£Œ í™•ì¸!"
          echo "âœ… ì¡°ì¸ í† í°ì´ Parameter Storeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
          echo "âœ… ì›Œì»¤ ë…¸ë“œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
          echo "=================================================="
          exit 0
        elif [ $? -ne 0 ]; then
          echo "â³ Parameter Store ì ‘ê·¼ ëŒ€ê¸° ì¤‘ (ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì§„í–‰ ì¤‘)"
        elif [ "$JOIN_CMD" = "placeholder" ]; then
          echo "â³ Parameter Storeì— placeholder ê°’ í™•ì¸ (ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì§„í–‰ ì¤‘)"
        else
          echo "â³ ì¡°ì¸ í† í° ìƒì„± ëŒ€ê¸° ì¤‘ (ë§ˆìŠ¤í„° ì´ˆê¸°í™” ì§„í–‰ ì¤‘)"
        fi
        
        # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ 10ì´ˆ ëŒ€ê¸°
        if [ $i -lt 90 ]; then
          echo "ğŸ• 10ì´ˆ í›„ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤..."
          sleep 10
        fi
      done
      
      echo "=================================================="
      echo "âŒ íƒ€ì„ì•„ì›ƒ: 15ë¶„ ë‚´ì— ë§ˆìŠ¤í„° ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
      echo "âŒ ë§ˆìŠ¤í„° ë…¸ë“œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”:"
      echo "   ssh -i ${var.cluster_name}-key.pem ec2-user@${aws_instance.k8s_master.public_ip}"
      echo "   tail -f /var/log/k8s-master-${local.cluster_id}.log"
      echo "=================================================="
      exit 1
    EOT
  }

  # ì„±ê³µ ì‹œ ì™„ë£Œ í‘œì‹œ íŒŒì¼ ìƒì„±
  provisioner "local-exec" {
    when    = create
    command = "echo 'MASTER_INIT_COMPLETE' > ${path.module}/master-init-complete.txt"
  }
}

#===========================================
# Kubernetes Worker Nodes
#===========================================

# Data source to ensure workers wait for master init completion
data "local_file" "master_init_complete" {
  filename = "${path.module}/master-init-complete.txt"
  depends_on = [null_resource.wait_for_master_init]
}

# Kubernetes Worker Nodes
resource "aws_instance" "k8s_workers" {
  count = var.worker_count

  ami                    = data.aws_ami.amazon_linux.id
  instance_type          = var.worker_instance_type
  key_name               = aws_key_pair.k8s_key.key_name
  vpc_security_group_ids = [aws_security_group.k8s_worker.id]
  subnet_id              = aws_subnet.public[count.index % length(aws_subnet.public)].id
  iam_instance_profile   = aws_iam_instance_profile.k8s_nodes.name

  root_block_device {
    volume_size           = var.disk_size
    volume_type           = "gp3"
    delete_on_termination = true
    encrypted             = false
  }

  metadata_options {
    http_tokens = "required"  # IMDSv2 only
  }

  user_data = base64encode(templatefile("${path.module}/scripts/k8s-worker-join.sh", {
    cluster_id        = local.cluster_id
    parameter_name    = aws_ssm_parameter.k8s_join_command.name
    aws_region        = var.aws_region
    cluster_name      = var.cluster_name
    worker_index      = count.index + 1
    master_init_check = data.local_file.master_init_complete.content
  }))

  tags = merge(local.common_tags, {
    Name = "${var.cluster_name}-worker-${count.index + 1}"
    Type = "Worker"
    Role = "DataPlane"
  })
}

#===========================================
# Real-time Log Monitoring
#===========================================

# ë§ˆìŠ¤í„° ë…¸ë“œ ë¡œê·¸ ìˆ˜ì§‘
resource "null_resource" "master_log_monitor" {
  depends_on = [aws_instance.k8s_master]

  # ì£¼ê¸°ì ìœ¼ë¡œ ë§ˆìŠ¤í„° ë¡œê·¸ ë‹¤ìš´ë¡œë“œ
  provisioner "local-exec" {
    command = <<-EOT
      mkdir -p ./logs
      
      # ë§ˆìŠ¤í„° ë…¸ë“œ ë¡œê·¸ ìˆ˜ì§‘ (ë°±ê·¸ë¼ìš´ë“œ)
      (
        echo "ğŸ“‹ ë§ˆìŠ¤í„° ë…¸ë“œ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘..."
        for i in {1..900}; do  # ìµœëŒ€ 30ë¶„ê°„ ëª¨ë‹ˆí„°ë§ (2ì´ˆ * 900 = 30ë¶„)
          sleep 2
          echo "[$i/900] ë§ˆìŠ¤í„° ë¡œê·¸ ìˆ˜ì§‘ ì¤‘... ($(date))"
          
          # SSHë¡œ ë¡œê·¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
          scp -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              ec2-user@${aws_instance.k8s_master.public_ip}:/var/log/k8s-master-${local.cluster_id}.log \
              ./logs/master-${local.cluster_id}.log 2>/dev/null || true
              
          scp -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              ec2-user@${aws_instance.k8s_master.public_ip}:/var/log/cloud-init-output.log \
              ./logs/master-cloud-init.log 2>/dev/null || true
          
          # cloud-init ì™„ë£Œ í™•ì¸
          if [ -f "./logs/master-cloud-init.log" ]; then
            if grep -q "Cloud-init.*finished" ./logs/master-cloud-init.log; then
              echo "âœ… ë§ˆìŠ¤í„° cloud-init ì™„ë£Œ ê°ì§€! ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
              break
            fi
          fi
          
          # ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ í™•ì¸ (ì¶”ê°€ ì²´í¬)
          ssh -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=5 \
              ec2-user@${aws_instance.k8s_master.public_ip} \
              "test -f /tmp/master-init-status && echo 'SCRIPT_COMPLETE'" 2>/dev/null | grep -q "SCRIPT_COMPLETE" && {
            echo "âœ… ë§ˆìŠ¤í„° ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ê°ì§€! ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
            break
          }
        done
        echo "ë§ˆìŠ¤í„° ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ"
      ) &
    EOT
  }
}

# ì›Œì»¤ ë…¸ë“œ ë¡œê·¸ ìˆ˜ì§‘
resource "null_resource" "worker_log_monitor" {
  count      = var.worker_count
  depends_on = [aws_instance.k8s_workers]

  provisioner "local-exec" {
    command = <<-EOT
      mkdir -p ./logs
      
      # ì›Œì»¤ ë…¸ë“œ ë¡œê·¸ ìˆ˜ì§‘ (ë°±ê·¸ë¼ìš´ë“œ)
      (
        echo "ğŸ“‹ ì›Œì»¤-${count.index + 1} ë…¸ë“œ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘..."
        for i in {1..900}; do  # ìµœëŒ€ 30ë¶„ê°„ ëª¨ë‹ˆí„°ë§ (2ì´ˆ * 900 = 30ë¶„)
          sleep 2
          echo "[$i/900] ì›Œì»¤-${count.index + 1} ë¡œê·¸ ìˆ˜ì§‘ ì¤‘... ($(date))"
          
          # SSHë¡œ ë¡œê·¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
          scp -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              ec2-user@${aws_instance.k8s_workers[count.index].public_ip}:/var/log/k8s-worker-${local.cluster_id}.log \
              ./logs/worker-${count.index + 1}-${local.cluster_id}.log 2>/dev/null || true
              
          scp -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=10 \
              ec2-user@${aws_instance.k8s_workers[count.index].public_ip}:/var/log/cloud-init-output.log \
              ./logs/worker-${count.index + 1}-cloud-init.log 2>/dev/null || true
          
          # cloud-init ì™„ë£Œ í™•ì¸
          if [ -f "./logs/worker-${count.index + 1}-cloud-init.log" ]; then
            if grep -q "Cloud-init.*finished" ./logs/worker-${count.index + 1}-cloud-init.log; then
              echo "âœ… ì›Œì»¤-${count.index + 1} cloud-init ì™„ë£Œ ê°ì§€! ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
              break
            fi
          fi
          
          # ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ í™•ì¸ (ì¶”ê°€ ì²´í¬)
          ssh -i ${var.cluster_name}-key.pem \
              -o StrictHostKeyChecking=no \
              -o ConnectTimeout=5 \
              ec2-user@${aws_instance.k8s_workers[count.index].public_ip} \
              "test -f /tmp/worker-join-status && echo 'SCRIPT_COMPLETE'" 2>/dev/null | grep -q "SCRIPT_COMPLETE" && {
            echo "âœ… ì›Œì»¤-${count.index + 1} ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ê°ì§€! ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
            break
          }
        done
        echo "ì›Œì»¤-${count.index + 1} ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ"
      ) &
    EOT
  }
}

# ë¡œê·¸ ìˆ˜ì§‘ ì•ˆë‚´
resource "null_resource" "log_collection_info" {
  depends_on = [
    null_resource.master_log_monitor,
    null_resource.worker_log_monitor
  ]

  provisioner "local-exec" {
    command = <<-EOT
      echo ""
      echo "=================================================="
      echo "ğŸ“‹ ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘ ì‹œì‘!"
      echo "=================================================="
      echo "ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: ./logs/"
      echo ""
      echo "ğŸ“ ìˆ˜ì§‘ë˜ëŠ” ë¡œê·¸ íŒŒì¼ë“¤:"
      echo "  ğŸ–¥ï¸  ë§ˆìŠ¤í„° ë…¸ë“œ:"
      echo "     - ./logs/master-${local.cluster_id}.log"
      echo "     - ./logs/master-cloud-init.log"
      echo ""
      echo "  âš™ï¸  ì›Œì»¤ ë…¸ë“œë“¤:"
      for i in {1..${var.worker_count}}; do
        echo "     - ./logs/worker-$i-${local.cluster_id}.log"
        echo "     - ./logs/worker-$i-cloud-init.log"
      done
      echo ""
      echo "ğŸ” ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸ ë°©ë²•:"
      echo "  tail -f ./logs/master-${local.cluster_id}.log"
      echo "  tail -f ./logs/worker-1-${local.cluster_id}.log"
      echo ""
      echo "ğŸ’¡ íŒ: ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ìœ„ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!"
      echo "=================================================="
      echo ""
    EOT
  }
}

#===========================================
# SSH Commands File Generation
#===========================================

# SSH ì ‘ì† ëª…ë ¹ì–´ë¥¼ íŒŒì¼ë¡œ ìë™ ì €ì¥
resource "local_file" "ssh_commands" {
  depends_on = [
    aws_instance.k8s_master,
    aws_instance.k8s_workers
  ]

  filename = "${path.module}/ssh_commands.txt"
  content = <<-EOT
================================================================
ğŸš€ Kubernetes Cluster SSH ì ‘ì† ëª…ë ¹ì–´
================================================================

ğŸ“‹ í´ëŸ¬ìŠ¤í„° ì •ë³´:
- í´ëŸ¬ìŠ¤í„° ì´ë¦„: ${var.cluster_name}
- í´ëŸ¬ìŠ¤í„° ID: ${local.cluster_id}
- SSH í‚¤ íŒŒì¼: ${var.cluster_name}-key.pem

================================================================
ğŸ–¥ï¸  ë§ˆìŠ¤í„° ë…¸ë“œ SSH ì ‘ì†:
================================================================
ssh -i ${var.cluster_name}-key.pem ec2-user@${aws_instance.k8s_master.public_ip}

================================================================
âš™ï¸  ì›Œì»¤ ë…¸ë“œ SSH ì ‘ì†:
================================================================
%{for i, instance in aws_instance.k8s_workers~}
# ì›Œì»¤ ë…¸ë“œ ${i + 1}
ssh -i ${var.cluster_name}-key.pem ec2-user@${instance.public_ip}

%{endfor~}
================================================================
ğŸ“ SSH Config ì„¤ì • (~/.ssh/configì— ì¶”ê°€):
================================================================

# Kubernetes Cluster: ${var.cluster_name}
Host ${var.cluster_name}-master
    HostName ${aws_instance.k8s_master.public_ip}
    User ec2-user
    IdentityFile ${path.module}/${var.cluster_name}-key.pem
    
%{for i, instance in aws_instance.k8s_workers~}
Host ${var.cluster_name}-worker-${i + 1}
    HostName ${instance.public_ip}
    User ec2-user
    IdentityFile ${path.module}/${var.cluster_name}-key.pem
    
%{endfor~}

================================================================
ğŸ¯ ì‚¬ìš© ë°©ë²•:
================================================================
1. SSH Config ì„¤ì • í›„:
   ssh ${var.cluster_name}-master
   ssh ${var.cluster_name}-worker-1

2. ì§ì ‘ ì ‘ì†:
   ìœ„ì˜ ssh ëª…ë ¹ì–´ ë³µì‚¬í•´ì„œ ì‚¬ìš©

3. í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸ (ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ):
   kubectl get nodes
   kubectl get pods -n kube-system

================================================================
ìƒì„±ì¼ì‹œ: $(date)
================================================================
  EOT

  file_permission = "0644"
}

#===========================================
# Cleanup on Destroy
#===========================================
# Cleanup local files when destroying infrastructure
resource "null_resource" "cleanup_on_destroy" {
  # destroy ì‹œì—ë§Œ ì‹¤í–‰ë˜ëŠ” ì •ë¦¬ ì‘ì—…
  provisioner "local-exec" {
    when    = destroy
    command = <<-EOT
      echo ""
      echo "ğŸ§¹ =============================================="
      echo "ğŸ§¹ ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ì‹œì‘..."
      echo "ğŸ§¹ =============================================="
      
      # ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (ë¡œê·¸ ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ë“¤)
      echo "ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘..."
      pkill -f "scp.*logs" 2>/dev/null || echo "â„¹ï¸  scp í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤"
      pkill -f "ssh.*ec2-user" 2>/dev/null || echo "â„¹ï¸  ssh í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤"
      pkill -f "tail.*logs" 2>/dev/null || echo "â„¹ï¸  tail í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤"
      
      # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
      echo "â³ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸° ì¤‘... (3ì´ˆ)"
      sleep 3
      
      # logs í´ë” ì‚­ì œ (ê°•ì œ)
      if [ -d "./logs" ]; then
        echo "ğŸ“ logs í´ë” ì‚­ì œ ì¤‘..."
        # íŒŒì¼ ì†ì„± ë³€ê²½ í›„ ê°•ì œ ì‚­ì œ
        chmod -R 755 ./logs 2>/dev/null || true
        rm -rf ./logs 2>/dev/null || {
          echo "âš ï¸  ì¼ë°˜ ì‚­ì œ ì‹¤íŒ¨, ê°•ì œ ì‚­ì œ ì‹œë„ ì¤‘..."
          sudo rm -rf ./logs 2>/dev/null || echo "âŒ logs í´ë” ì‚­ì œ ì‹¤íŒ¨ (ìˆ˜ë™ ì‚­ì œ í•„ìš”)"
        }
        
        if [ ! -d "./logs" ]; then
          echo "âœ… logs í´ë” ì‚­ì œ ì™„ë£Œ"
        else
          echo "âš ï¸  logs í´ë”ê°€ ì•„ì§ ì¡´ì¬í•©ë‹ˆë‹¤ (íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŒ)"
        fi
      else
        echo "â„¹ï¸  logs í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
      fi
      
      # ì™„ë£Œ íŒŒì¼ ì‚­ì œ
      if [ -f "./master-init-complete.txt" ]; then
        echo "ğŸ“„ ì™„ë£Œ íŒŒì¼ ì‚­ì œ ì¤‘..."
        rm -f ./master-init-complete.txt 2>/dev/null || echo "âš ï¸  ì™„ë£Œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨"
        echo "âœ… ì™„ë£Œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ"
      else
        echo "â„¹ï¸  ì™„ë£Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
      fi
      
      # SSH ëª…ë ¹ì–´ íŒŒì¼ ì‚­ì œ
      if [ -f "./ssh_commands.txt" ]; then
        echo "ğŸ“„ SSH ëª…ë ¹ì–´ íŒŒì¼ ì‚­ì œ ì¤‘..."
        rm -f ./ssh_commands.txt 2>/dev/null || echo "âš ï¸  SSH ëª…ë ¹ì–´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨"
        echo "âœ… SSH ëª…ë ¹ì–´ íŒŒì¼ ì‚­ì œ ì™„ë£Œ"
      else
        echo "â„¹ï¸  SSH ëª…ë ¹ì–´ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
      fi
      
      # SSH í‚¤ íŒŒì¼ë“¤ ì •ë¦¬
      if ls ./*-key.pem 1> /dev/null 2>&1; then
        echo "ğŸ”‘ SSH í‚¤ íŒŒì¼ ì •ë¦¬ ì¤‘..."
        rm -f ./*-key.pem 2>/dev/null || echo "âš ï¸  SSH í‚¤ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨"
        echo "âœ… SSH í‚¤ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ"
      else
        echo "â„¹ï¸  SSH í‚¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
      fi
      
      # ë°±ì—… íŒŒì¼ë“¤ ì •ë¦¬
      if ls ./*.backup 1> /dev/null 2>&1; then
        echo "ğŸ’¾ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì¤‘..."
        rm -f ./*.backup 2>/dev/null || echo "âš ï¸  ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨"
        echo "âœ… ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ"
      else
        echo "â„¹ï¸  ë°±ì—… íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
      fi
      
      # ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬ (ì¶”ê°€)
      echo "ğŸ—‘ï¸  ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬ ì¤‘..."
      rm -f ./*.tmp 2>/dev/null || true
      rm -f ./*.log 2>/dev/null || true
      rm -f ./*.pid 2>/dev/null || true
      rm -f ./terraform.tfstate.backup* 2>/dev/null || true
      
      # ìˆ¨ê¹€ íŒŒì¼ë“¤ ì •ë¦¬
      rm -f ./.terraform.lock.hcl.backup 2>/dev/null || true
      
      # ìµœì¢… í™•ì¸
      echo "ğŸ” ì •ë¦¬ ê²°ê³¼ í™•ì¸..."
      REMAINING_FILES=$(ls -la . 2>/dev/null | grep -E "(logs|\.log|\.tmp|key\.pem|ssh_commands\.txt|master-init-complete\.txt)" | wc -l)
      if [ "$REMAINING_FILES" -eq 0 ]; then
        echo "âœ… ëª¨ë“  ëŒ€ìƒ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤"
      else
        echo "âš ï¸  ì¼ë¶€ íŒŒì¼ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤:"
        ls -la . 2>/dev/null | grep -E "(logs|\.log|\.tmp|key\.pem|ssh_commands\.txt|master-init-complete\.txt)" || true
        echo "ğŸ’¡ VSCodeì—ì„œ íŒŒì¼ì„ ì—´ê³  ìˆê±°ë‚˜ ë°±ê·¸ë¼ìš´ë“œ í”„ë¡œì„¸ìŠ¤ê°€ ì‚¬ìš© ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
      fi
      
      echo ""
      echo "ğŸ¯ =============================================="
      echo "ğŸ¯ ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ!"
      echo "ğŸ¯ =============================================="
      echo ""
    EOT
  }

}